{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVrrpnrYsw7k",
        "outputId": "08329dfb-99cf-4187-aba8-43ccdddf791d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting confluent-kafka\n",
            "  Downloading confluent_kafka-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: confluent-kafka\n",
            "Successfully installed confluent-kafka-2.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install confluent-kafka"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YhYNgihChCr",
        "outputId": "a774660f-c347-4637-c4e1-519ac2d807c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Faker\n",
            "  Downloading Faker-18.9.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from Faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->Faker) (1.16.0)\n",
            "Installing collected packages: Faker\n",
            "Successfully installed Faker-18.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Producer, Consumer, KafkaError\n",
        "from faker import Faker\n",
        "import json"
      ],
      "metadata": {
        "id": "zZs3QM0vtHOR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake = Faker('en_US')"
      ],
      "metadata": {
        "id": "yWi5-e-wFeDU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Kafka producer instance\n",
        "producer_config = {\n",
        "    'bootstrap.servers': 'pkc-lzvrd.us-west4.gcp.confluent.cloud:9092',\n",
        "    'security.protocol': 'SASL_SSL',\n",
        "    'sasl.mechanisms': 'PLAIN',\n",
        "    'sasl.username': 'Z2W5M7OMMQG3IH6I',\n",
        "    'sasl.password': 'Y68+Y6NHrTpMzq/ETj9tj5m1K7kScjdA1dc1aO6L516S9qGTqGX09gNXk/Ol7c+Q',  \n",
        "}"
      ],
      "metadata": {
        "id": "J6gRjIHptJBI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Kafka cosumer instance\n",
        "consumer_conf = {\n",
        "    'bootstrap.servers': 'pkc-lzvrd.us-west4.gcp.confluent.cloud:9092',\n",
        "    'security.protocol': 'SASL_SSL',\n",
        "    'sasl.mechanisms': 'PLAIN',\n",
        "    'sasl.username': 'Z2W5M7OMMQG3IH6I',\n",
        "    'sasl.password': 'Y68+Y6NHrTpMzq/ETj9tj5m1K7kScjdA1dc1aO6L516S9qGTqGX09gNXk/Ol7c+Q',\n",
        "    'group.id': 'cluster_0',\n",
        "    'auto.offset.reset': 'earliest'\n",
        "}\n",
        "\n",
        "consumer = Consumer(consumer_conf)"
      ],
      "metadata": {
        "id": "VcJRjrME0qWB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic = 'sample_data'"
      ],
      "metadata": {
        "id": "FkllHc9YNwZN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a function for indegisting Data into a Kafka Cluster\n",
        "\n",
        "def Produce():\n",
        "\n",
        "  #Define number of messages to generate\n",
        "    num_messages = 20\n",
        "          \n",
        "    # Initialize Kafka producer\n",
        "    producer = Producer(producer_config)\n",
        "\n",
        "\n",
        "    #Create Synthetic data to be ingested into Kafka Cluster\n",
        "    for i in range(num_messages):\n",
        "        data = {\n",
        "            'transaction_id':fake.random_int(),\n",
        "            'sender_phone_number': fake.phone_number(),\n",
        "            'receiver_phone_number': fake.phone_number(),\n",
        "            'transaction_amount': fake.random_int(),\n",
        "            'transaction_time\"':fake.time()\n",
        "        }\n",
        "        \n",
        "        # Serialize data to JSON and send to Kafka topic\n",
        "        producer.produce(topic, key=str(i), value=json.dumps(data))\n",
        "        producer.flush()"
      ],
      "metadata": {
        "id": "PXWXVJARHEOT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a function for Extracting Data from Kafka Cluster\n",
        "\n",
        "def Extract():\n",
        "\n",
        "# Create Kafka consumer and subscribe to input topic\n",
        "\n",
        "   consumer = Consumer(consumer_conf)\n",
        "   consumer.subscribe([topic])\n",
        "\n",
        "#Define an empty list to store the JSON data\n",
        "json_data_list = []\n",
        "\n",
        "\n",
        "#Define max messages and message count variables so as to create a condition to exit the While loop\n",
        "max_messages = 20\n",
        "message_count = 0\n",
        "\n",
        "# Consume data from Kafka\n",
        "while True:\n",
        "        msg = consumer.poll(timeout=1.0)\n",
        "        if msg is None:\n",
        "            continue\n",
        "        if msg.error():\n",
        "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "                print('Reached end of partition')\n",
        "            else:\n",
        "                print('Error while consuming message: {}'.format(msg.error()))\n",
        "        else:\n",
        "            # Convert the message value from bytes to string\n",
        "            json_string = msg.value().decode('utf-8')\n",
        "            \n",
        "            # Parse the JSON data\n",
        "            json_data = json.loads(json_string)\n",
        "            \n",
        "            # Append the JSON data to the list\n",
        "            json_data_list.append(json_data)\n",
        "            \n",
        "            #Increment the message counter\n",
        "            message_count += 1\n",
        "\n",
        "            # Exit the loop when we have consumed the maximum number of messages set in max_messages variable\n",
        "            if message_count >= max_messages:\n",
        "                break"
      ],
      "metadata": {
        "id": "SYad-8ErQlEk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}